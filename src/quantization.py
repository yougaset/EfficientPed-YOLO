import os
import torch
from torch.quantization import QuantStub, DeQuantStub, prepare_qat, convert
from ultralytics import YOLO

# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆ`src` çš„ä¸Šä¸€çº§ç›®å½•ï¼‰
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# è®­ç»ƒå®Œæˆçš„æ¨¡å‹è·¯å¾„ï¼ˆç¡®ä¿æŒ‡å‘ `results/` æ ¹ç›®å½•ï¼‰
MODEL_PATH = os.path.normpath(os.path.join(BASE_DIR, "results", "yolov8_citypersons_improved", "weights", "best.pt"))

# é‡åŒ–åæ¨¡å‹çš„ä¿å­˜è·¯å¾„ï¼ˆå­˜æ”¾åœ¨ `results/models/`ï¼‰
QUANTIZED_MODEL_PATH = os.path.normpath(os.path.join(BASE_DIR, "results", "models", "best_quantized.pt"))

# ç¡®ä¿ `results/models/` ç›®å½•å­˜åœ¨
os.makedirs(os.path.dirname(QUANTIZED_MODEL_PATH), exist_ok=True)


# é‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ¨¡å‹
class QuantizedModel(torch.nn.Module):
    def __init__(self, model):
        super(QuantizedModel, self).__init__()
        self.quant = QuantStub()  # é‡åŒ–è¾“å…¥
        self.model = model  # åŸ YOLO æ¨¡å‹
        self.dequant = DeQuantStub()  # åé‡åŒ–

    def forward(self, x):
        x = self.quant(x)  # é‡åŒ–è¾“å…¥
        x = self.model(x)  # é€šè¿‡æ¨¡å‹
        x = self.dequant(x)  # åé‡åŒ–è¾“å‡º
        return x


# é‡åŒ–æµç¨‹
def quantize_model():
    """åŠ è½½ YOLO è®­ç»ƒæ¨¡å‹ï¼Œå¹¶è¿›è¡Œ QAT é‡åŒ–"""

    # âœ… ç¡®ä¿ `best.pt` å­˜åœ¨
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_PATH}ï¼Œè¯·å…ˆè¿è¡Œ train.py è¿›è¡Œè®­ç»ƒï¼")

    print(f"âœ… åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    model = YOLO(MODEL_PATH)

    # åŒ…è£…ä¸ºé‡åŒ–æ¨¡å‹
    quantized_model = QuantizedModel(model.model)
    quantized_model.qconfig = torch.quantization.get_default_qat_qconfig("fbgemm")

    # **ä¿®æ­£ QAT è®¾ç½®ï¼Œç¡®ä¿ Conv2d & Linear æ¨¡å—å‚ä¸é‡åŒ–**
    for name, module in quantized_model.named_modules():
        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):
            module.qconfig = torch.quantization.default_qat_qconfig

    # å‡†å¤‡é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰
    prepare_qat(quantized_model, inplace=True)
    print("ğŸ“Œ é‡åŒ–æ„ŸçŸ¥è®­ç»ƒå·²å‡†å¤‡å®Œæˆï¼Œå¼€å§‹è½¬æ¢...")

    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    quantized_model = convert(quantized_model, inplace=True)
    print("âœ… é‡åŒ–è½¬æ¢å®Œæˆï¼")

    # ä¿å­˜é‡åŒ–æ¨¡å‹
    torch.save(quantized_model.state_dict(), QUANTIZED_MODEL_PATH)
    print(f"âœ… é‡åŒ–åçš„æ¨¡å‹å·²ä¿å­˜è‡³: {QUANTIZED_MODEL_PATH}")


if __name__ == "__main__":
    quantize_model()
